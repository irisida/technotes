# Docker

## Image layering

We encounter our first potential for Dockerfile optimisation when we talk about image layering. Basically each command in a dockerfile is treated like a layer that builds up the image. While we can advantage from cached layers if we rebuild the exact same image and see the build take a fraction of a second, where anything changes a full rebuild and new image is created.

Where we have a change in one layer (say, source code) all subsequent layers are re-run afterwards because Docker doesn't do any deep analysis. It notices that in the steps of commands one has a difference to the previous build and therefore all further steps must be treated as if there is no history and re-run. In our file we do the copy of source before the `npm install` so any source code changes and rebuild will also run the `npm install` which is inefficient. To circumvent that we can do the following

![](/assets/docker/docker01.png)

## Inspect an image

with `docker image inspect imageId` it's possible to inspect the image and see the construction attributes. ie. Exposed ports, environment version variables. It's also holding the underlying OS layers. It also details the actual layers of the image itself which are part of the build/rebuild and caching utilisation.

## Copying files from local into a container

We can copy files from the local project directory into a running container using the standard `cp` commands. We use the format `docker cp targetFolder/targetFile containerName:/destinationFolder`
It' also possible to provide the path to a folder and take all the files by using a dot as a the all indicator. `docker cp targetFolder/. containerName:/destination`

## Copy from container to local

work by placing the parameters from above in opposite order. `docker cp containerFolder/filename/. localProjectFolder/`

## adding tags to an image at runtime

Where no name for an image is provided one will be generated by docker, however, these are normally quite stupid, not descriptive and can be annoying to type. We can provide a tag at the time of running by using: `docker run -p 3000:80 -d --rm --name descriptiveName containerId`

## adding names:tags to an image at build time

we can add names and tags in he format of `name:tag` to an image being built. To do so use `docker build -t name:tag .` to generate a named and tagged build.

## Sharing Images and Containers

Everyone who has an image can create containers based on that image. This means that we don't typically share containers, instead we share images from which a container can be built. There are two main way to share:

1. We share the dockerfile and the source code, this allows someone we are sharing with to run a `docker build .` on the assets we have shared and allowing them to construct the image.
2. We share the image we have built and people create a running container based on that image.

Dockerhub is considered the default hub, if you want to push to/pull from a private repository hub you have to specify the location in the push/pull commands. Authentication is required and a one-time username/password combo will be required at the terminal. Once successfully logged in Docker will push your image in a smart capacity, ie an image based on the official node repo wont upload all of that code as it is already on dockerhub. This saves space on their side and bandwith on the users side and is therefore a win-win of a feature.

The default approach when pulling from a Docker registry is to take the latest.

If you `docker run username/repositoryName` and docker does not find it locally in the history it will reach out to dockerhub and pull that automatically. One caveat is that where an image is held locally, docker will **not** automatically check for updates on dockerhub. Where you require to be guaranteed to be running the latest it is better to use `docker pull username/repositoryName` which will take the new images if updated and use the local one you already have if it has not been updated since the local one was pulled.
